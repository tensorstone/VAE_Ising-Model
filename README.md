# VAE_Ising-Model
use variational auto-encoders to deal with Ising models


Week 1: 3rd-9th,July
----
### 1. introduction
When using Markov Chain Monte Carlo method, the sampling method is of crucial for the simulation efficiency. Only when the flip steps are repeated for enough times can the system reaches equilibrium. To generate enough independent samples at each temperature, the flip step should be repeated for more than ~M * N times, where M is the number of independent samples and N=I*J is the square lattice's grid scale.

Here I want to find a more efficient way to generate such simulated configurations using VAEs and GANs.

### 2. Here are some previous work:
1.[1410.3831v1]An exact mapping between the Variational Renormalization Group and Deep Learning. The author showed the relationship between renormalization group(RG) and Restricted Boltzmann Machines(RBM) in Ising model.

2.[1606.00318]Discovering phase transitions with unsupervised learning. The author used principal component analysis(PCA) to find order parameters of Ising model.

3.[1605.01735]Machine learning phases of matter. The author identified phases and phase transitions in condensed matter systems via supervised machine learning.

4.[1703.02435]Unsupervised learning of phase transitions: from principal component analysis to variational autoencoders. The author used PCA, AE and VAE to learn about the latent parameters which best describe states of the two-dimensional Ising model and the three-dimensional XY model.

### 3. AE/VAEs for Ising model
The configuration generated by VAEs in 1703.02435 can not approximate the real images from a mathematical perspective. This week I concentrate on finding the reason of this phenomenon.

I duplicated an AE mentioned in the paper. In the figure below, the X-axis is the hidden variable in the hidden layer of my VAE and the Y-axis is the magnetic moment.

<img src="https://github.com/tensorstone/VAE_Ising-Model/blob/master/A%20line%E4%B8%80%E6%9D%A1%E7%9B%B4%E7%BA%BF.png" width=350 height=350 />

![image](https://github.com/tensorstone/VAE_Ising-Model/blob/master/Generated_VAE%E7%94%9F%E6%88%90.png)

When I used the sign function to duplecate a configuration at a certain temperature, I got this

![image](https://github.com/tensorstone/VAE_Ising-Model/blob/master/Gen2.png)
![image](https://github.com/tensorstone/VAE_Ising-Model/blob/master/Gen1.png)

Obviously, VAE itself performs well in classification tasks, but performs poorly in replication tasks.

### 4. Math perspective
![image](https://github.com/tensorstone/VAE_Ising-Model/blob/master/NetVisualize.png)

Here the KL-Divergence(KL-term for short) constrain the mapping process of each T from the initial distribution to a standard Normal Distribution, and the Binary_ term means binary_crossentropy. 

Different configurations in each T map to different mu and sigma, then sample a new point from this normal distribution. The first figure in this part shows that the hidden variable is approximately an uniform distribution. The reason is our Ts are continous, so that the configurations of near Ts are hard to be separated.(Many Gauss distributions consist of a uniform distribution)

We sample hidden variables near z_mean. To reproduce the initial configuration at high accuracy, we must separate different T's z_mean_Ts. Here the Binary_ loss plays its role. 

In this framework, the loss function lead to a result of the separation of the experimental samples at various temperatures. But if the aim is reproduce a fake but true figure, we shall use other loss functions.

Week 2: 10th-16th,July
----
### 5. More from mathematical perspective

In AEs, the optimization objective is to minimize the cross entropy between the inputs and the outputs. A certain configuration is mapped to a certain point in the hidden layer, which turns out to be the magnetic moment.

In VAEs, the optimization objective is to minimize the corss entropy between the inputs and the configuration generated from a Gaussian re-sample process in the hidden layer. So, a certain configuration is mapped to an area with Gaussian probability density. Such configuration may be re-sampled to each of those points, and vise versa. It's because such configuration may come from any magnetic moment those points represent. This leads to an expansion in the hidden layer-magnetic moment graph.

<img src="https://github.com/tensorstone/VAE_Ising-Model/blob/master/ae_ising.png" width=350 height=350 /><img src="https://github.com/tensorstone/VAE_Ising-Model/blob/master/vae_ising_1.png" width=350 height=350 />

If we can sample from a Gaussian distribution with great variance and reproduce the initial configuration, we may draw a conclusion that the configuration within this Gaussian should be alike. Sothat the variance here represents the extent of the magnetic moment near this temperature. We may find the critical temperature clearly from this figure:


<img src="https://github.com/tensorstone/VAE_Ising-Model/blob/master/vae_ising_var.png" width=350 height=350 />


So, if we set the target to precisely produce a picture near a certain temperature, we should use AEs instead of VAEs.

### 6. Improvement: loss function

[https://github.com/tensorstone/VAE_Ising-Model/blob/master/AE%20Ising%20and%20collapse.ipynb]

Now our aim changes from classification to reproduction, we may use our knowledge in physics to improve the precision.So first, I chose to use a new loss function of

<pre><code>Loss_M = M_input - M_output</code></pre>

 (To make this process seem more intelligent, we can use hidden variabels instead of M. But actually it's just a linear transformation, as is shown above.)

I also add another loss to the loss function:

<pre><code>Loss_constrain = -Sum((Output - 0.5)^2)</code></pre>

which works as a penalty term, and restrict the value of each spin to 0 or 1.

<pre><code>Loss = Loss_M + Loss_constrain</code></pre>

***[What's wrong?]***

One more experiment is about using the M itself to generate configurations. But failed even with transfer learning technics. It appears to converge to stable local minima easily.

[https://github.com/tensorstone/VAE_Ising-Model/blob/master/Generate%20with%20T%20and%20M.ipynb]

### 7. Scale expansion
Add some output units for scale expansion. But the problem now is mode collapse. Maybe I can try GAN to solve such problem

see https://github.com/tensorstone/VAE_Ising-Model/blob/master/Week%202.pdf

### 8. Other results and problems
Problems now come from the generator.

Simple generator may lead to totally random configurations

Complex ones will lead to mode collapse (even 2 layers)

But luckily found a standard for measuring results, inspired by Ce Wang.

Correlationg function.

I think this can also be used as a standard for GANs.
